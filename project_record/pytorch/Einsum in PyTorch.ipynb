{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EINSUM IN DEEP LEARNING once for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einstein summation 爱因斯坦求和在numpy中的实现 [einsum in numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html)，在TensorFlow中的实现[einsum in tensorflow](https://www.tensorflow.org/api_docs/python/tf/einsum)以及在PyTorch中的实现[einsum in PyTorch](http://pytorch.org/docs/master/torch.html?highlight=torch%20max#torch.einsum).相关Einstein summation解释的博客，[Olexa](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/)[Alex](http://ajcr.net/Basic-guide-to-einsum/).\n",
    "\n",
    "Einstein summation可以很好的替代TensorFlow或PyTorch中的dot products, outer products, transposes 以及 matrix-vector 或 matrix-matrix乘法函数实现，即可以简洁的实现上述常用计算操作,实现高效编码。einsum是一种domain-specific language 域指定语言,可以实现高性能的代码实现。einsum的语法基于PyTorch中的[ensor Comprehensions](http://pytorch.org/2018/03/05/tensor-comprehensions.html),可以自动产生GPU代码以及微调代码来适应特定大小的输入。[opt insum](https://github.com/dgasmith/opt_einsum)和[tf einsum opt](https://github.com/Bihaqo/tf_einsum_opt)可用于优化tensor收缩顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EINSUM 注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnmPy、PyTorch以及TensorFlow中的爱因斯坦求和"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy中的实现:np.einsum; PyTorch实现:torch.einsum; TensorFlow中的实现:tf.einsum,三个函数的签名均为```einsum(equation, operands)```, equation为爱因斯坦求的字符串表达，operands 操作数为操作的目标ensor。eg: ${\\color{green}c_j} = \\sum_i\\sum_k {\\color{red}A_{ik}}{\\color{blue}B_{kj}}$可以用equation string(方程式字符串)表示为\"${\\color{red}ik},{\\color{blue}kj}$->${\\color{green}j}$\", note:其中i,j,k索引的指示可以为任意字母，但必须持续保持。\n",
    "\n",
    "爱因斯坦求和还可以用来计算神经网络中任意计算图并且可以反向传播。\n",
    "![image.png](attachment:image.png)\n",
    "其中双引号中方块是指向tensor具体维度的占位符，可以看出arg1、arg3为2-D tensor,即二维矩阵,arg3为3-D tensor,结果result为2-D matric。einsum()函数的输入可以为任意个不同维度的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eg: 1.Matrix Transpose 矩阵转置                 \n",
    "${\\color{green}B_{ji}} = {\\color{red}A_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.arange(6).reshape(2,3)\n",
    "a_ = torch.einsum('ij->ji', [a]) #此操作不会更改a的值，即该操作是对a的副本进行。\n",
    ">>> a = torch.arange(6).reshape(2,3)\n",
    ">>> a\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]])\n",
    ">>> torch.einsum(\"ij->ji\", a)\n",
    "tensor([[0, 3],\n",
    "        [1, 4],\n",
    "        [2, 5]])\n",
    ">>> a\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.SUM 求和操作\n",
    "${\\color{green}b} = \\sum_i\\sum_j {\\color{red}A_{ij}} = {\\color{red}A_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2,3)\n",
    "a_ = torch.einsum(\"ij->\", a) #求和操作，结果为一个整数值\n",
    ">>> a_ = torch.einsum(\"ij->\",a)\n",
    ">>> a_\n",
    "tensor(15)\n",
    ">>> a_.size()\n",
    "torch.Size([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Column SUM 行求和\n",
    "${\\color{green}b_j} = \\sum_i{\\color{red}A_{ij}} = {\\color{red}A_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a_ = torch.einsum(\"ij->j\", a) #保持列维度\n",
    ">>> a_\n",
    "tensor([3, 5, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Row SUM 列求和\n",
    "${\\color{green}b_i} = \\sum_j{\\color{red}A_{ij}} = {\\color{red}A_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a_ = torch.einsum('ij -> i', a)\n",
    ">>> a_\n",
    "tensor([ 3, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix-Vector Multiplication 矩阵-向量 乘法\n",
    "${\\color{green}c_i} = \\sum_k {\\color{red} A_{ik}}{\\color{blue} b_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]])\n",
    ">>> b = torch.arange(3)\n",
    ">>> b\n",
    "tensor([0, 1, 2])\n",
    ">>> a_ = torch.einsum('ij,j -> i', [a, b])\n",
    ">>> a_\n",
    "tensor([ 5, 14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrix-Matrix Multiplication 矩阵-矩阵 乘法\n",
    "${\\color{green}c_{ij}} = \\sum_k {\\color{red}A_{ik}}{\\color{blue}B_{kj}} = {\\color{red}A_{ik}}{\\color{blue}B_{kj}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]])\n",
    ">>> b\n",
    "tensor([[ 0,  1,  2,  3,  4],\n",
    "        [ 5,  6,  7,  8,  9],\n",
    "        [10, 11, 12, 13, 14]])\n",
    ">>> a_ = torch.einsum('ik,kj -> ij', [a, b])\n",
    ">>> a_\n",
    "tensor([[ 25,  28,  31,  34,  37],\n",
    "        [ 70,  82,  94, 106, 118]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dot Product 点积操作\n",
    "（vector 向量点乘）\n",
    "${\\color{green}c} = \\sum_i {\\color{red}a_i\\color{blue}b_i} = {\\color{red}a_i\\color{blue}b_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(3)\n",
    "b = torch.arange(3,6)  # -- a vector of length 3 containing [3, 4, 5]\n",
    "a_ = torch.einsum('i,i->', [a, b])\n",
    ">>> a_\n",
    "tensor(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Matrix 矩阵点乘)\n",
    "   ${\\color{green}c} = \\sum_i\\sum_j {\\color{red}A_{ij}\\color{blue}B_{ij}} = {\\color{red}A_{ij}\\color{blue}B_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "a_ = torch.einsum('ij,ij->', [a, b])\n",
    ">>> a_\n",
    "tensor(145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hadamard Product 哈达马乘积\n",
    "${\\color{green}C_{ij}} = {\\color{red}A_{ij}}{\\color{blue}B_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.arange(6).reshape(2,3)\n",
    ">>> a\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5]])\n",
    "b = torch.arange(6, 12).reshape(2,3)\n",
    "tensor([[ 6,  7,  8],\n",
    "        [ 9, 10, 11]])\n",
    "a_ = torch.einsum('ij,ij->ij', [a,b])\n",
    ">>> a_\n",
    "tensor([[ 0,  7, 16],\n",
    "        [27, 40, 55]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  einsum与PyTorch操作的替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn([4, 3, 224, 224]) #bcwh\n",
    "b = torch.randn([4, 3, 224, 224]) #bcwh\n",
    "a_ = torch.einsum('bcwh -> bc', [a])\n",
    "## a_ 的替换写法\n",
    "a_ = torch.zeros(a.shape[0], a.shape[1])\n",
    "for vi in range(a.shape[0]):\n",
    "    for vj in range(a.shape[1]):\n",
    "        a_[vi][vj] = torch.sum(a[vi,vj,...]) #equal to torch.sum(a[vi,vj,:,:])\n",
    "        \n",
    "        \n",
    "a_ = torch.einsum('bcwh, bcwh -> bc', [pc, tc])\n",
    "## a_的替换写法\n",
    "temp = F.mul(a, b)\n",
    "intersection_ = torch.zeros(a.shape[0], b.shape[1])\n",
    "for vi in range(a.shape[0]):\n",
    "    for vj in range(a.shape[1]):\n",
    "        intersection_ = torch.sum(temp[vi, vj,:,:])\n",
    "        \n",
    "a_ = torch.einsum('bc -> b', [intersection_])\n",
    "a_ = torch.sum(intersection_, 1) #1 for cols, 0 for rows\n",
    ">>> a_.size()\n",
    "torch.Size([4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outer Product 外积（线性代数）\n",
    "${\\color{green}C_{ij}} = {\\color{red}a_i\\color{blue}b_j}$ \n",
    "\n",
    "\n",
    "note:Outer Product 指张量积；Exterior Product是指解析几何中的外积[Exterior product](https://en.wikipedia.org/wiki/Exterior_algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a = torch.arange(3)\n",
    ">>> b = torch.arange(3,7)\n",
    ">>> b\n",
    "tensor([3, 4, 5, 6])\n",
    ">>> a_ = torch.einsum('i, j -> ij', [a,b])\n",
    ">>> a_\n",
    "tensor([[ 0,  0,  0,  0],\n",
    "        [ 3,  4,  5,  6],\n",
    "        [ 6,  8, 10, 12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Matrix Multiplication\n",
    "${\\color{green}C_{ijl}} = \\sum_k{\\color{red}A_{ijk}\\color{blue}B_{ikl}} = {\\color{red}A_{ijk}\\color{blue}B_{ikl}}$\n",
    "\n",
    "note:遵循矩阵乘法原则 \n",
    "$A_{3x{\\color{red}{2x5}}}B_{3x{\\color{red}{5x3}}}=C_{3x{\\color{red}{2x3}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a = torch.randn(3,2,5)\n",
    ">>> a\n",
    "tensor([[[ 1.0289, -0.1355, -0.4099, -1.4501, -0.5968],\n",
    "         [-0.1667, -0.0286,  2.4426, -1.6169, -1.4544]],\n",
    "\n",
    "        [[ 0.6575, -0.4081, -0.3920, -1.5732, -0.2900],\n",
    "         [ 0.8656, -0.1987,  0.6598,  0.6242,  0.8018]],\n",
    "\n",
    "        [[ 0.5719,  0.0117,  0.6081,  0.0559, -1.6444],\n",
    "         [ 0.6912, -1.0747, -0.6326, -0.7700,  0.4683]]])\n",
    ">>> b = torch.randn(3,5,3)\n",
    ">>> b\n",
    "tensor([[[-0.8491,  0.2271,  0.3570],\n",
    "         [-2.1355, -0.2560,  1.1238],\n",
    "         [ 2.1473, -0.7215, -0.1438],\n",
    "         [ 1.0483,  0.0798,  0.6447],\n",
    "         [-0.0853,  1.1740, -0.9754]],\n",
    "\n",
    "        [[ 0.0629,  0.5650,  0.6519],\n",
    "         [ 0.3167, -0.1258, -0.3802],\n",
    "         [-1.8143, -1.5648,  0.8874],\n",
    "         [-0.6165,  0.7251,  0.5943],\n",
    "         [ 1.5743, -1.2373, -2.3647]],\n",
    "\n",
    "        [[-1.9334, -3.4358,  0.7900],\n",
    "         [ 1.5289,  0.7921, -0.7569],\n",
    "         [ 1.5598,  0.1017, -0.2741],\n",
    "         [-2.2164, -1.4216,  0.6508],\n",
    "         [-0.3690, -0.0338, -1.3386]]])\n",
    ">>> a_ = torch.einsum('ijk, ikl -> ijl', [a, b]) #遵循矩阵乘法原则\n",
    ">>> a_\n",
    "tensor([[[-2.9337, -0.2524, -0.0787],\n",
    "         [ 3.8770, -3.6294, -0.0668]],\n",
    "\n",
    "        [[ 1.1368,  0.2542, -0.0134],\n",
    "         [-0.3282, -1.0579, -0.2997]],\n",
    "\n",
    "        [[ 0.3435, -1.9180,  2.5137],\n",
    "         [-2.4323, -2.2114,  0.4050]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tensor Contraction 张量收缩 (多维矩阵相乘)\n",
    "${\\color{green}C_{pstuv}} = \\sum_q\\sum_r{\\color{red}A_{pqrs}\\color{blue}B_{tuqvr}} = {\\color{red}A_{pqrs}\\color{blue}B_{tuqvr}}$\n",
    "\n",
    "note:10中的Batch Matrix Multiplicationui是 [Tensor Contraction](https://en.wikipedia.org/wiki/Tensor_contraction)的一种特例。栗子：n-D 维度有序的tensor ${\\color{red}{A(I_1,...,I_n)}}$,  m-D 维度有序的tensor${\\color{blue}{B(J_1,...,J_m)}}$, Assume $n=4$,$m=5$, 且$I_2=J_3 \\quad and \\quad I_3=I_5$, 则可以对$A$矩阵中的第2和第3,对$B$矩阵中的第3和第5维度进行张量$A$和张量$B$的tensor multiply张量乘法操作(张量收缩),最后结果可以得到如下的张量${\\color{green}{C({I_1}\\times{I_4}\\times{J_1}\\times{J_2}\\times{J_4})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(2,3,5,7)\n",
    "b = torch.randn(11,13,3,17,5)\n",
    "torch.einsum('pqrs,tuqvr->pstuv', [a, b]).shape\n",
    ">>>torch.Size([2, 7, 11, 13, 17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Bilinear Transformation 双线性变换  $(y=x_1Ax_2))$\n",
    "${\\color{green}D_{ij}} = \\sum_k\\sum_l{\\color{red}A_{ik}}{\\color{purple}B_{jkl}}{\\color{blue}C_{il}} = {\\color{red}A_{ik}}{\\color{purple}B_{jkl}}{\\color{blue}C_{il}}$\n",
    "\n",
    "note:einsum可以操作多于两个的tensors，[bilinear transformation](https://pytorch.org/docs/master/nn.html#torch.nn.Bilinear) 双线性变换就是一个例子。\n",
    "\n",
    "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)\n",
    "\n",
    "Parameters:\t\n",
    "in1_features – size of each first input sample\n",
    "\n",
    "in2_features – size of each second input sample\n",
    "\n",
    "out_features – size of each output sample\n",
    "\n",
    "bias – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> a = torch.randn(2,3)\n",
    ">>> a\n",
    "tensor([[ 0.6125,  0.2316, -0.6014],\n",
    "        [-0.8615, -0.9863,  1.2960]])\n",
    ">>> b = torch.randn(5, 3, 7)\n",
    ">>> b\n",
    "tensor([[[-1.1676,  0.0660, -0.8437, -0.1520, -2.5223, -0.2943, -1.2686],\n",
    "         [-0.6428,  1.5299, -1.0441,  1.1833,  0.3706,  0.6328,  0.6845],\n",
    "         [-0.0108, -0.6111, -0.8319, -0.6406, -0.9033, -0.7832, -0.1204]],\n",
    "\n",
    "        [[-1.1418,  0.6051,  0.2700,  0.4359,  0.5383,  1.5829,  0.0230],\n",
    "         [ 1.4027, -0.3302,  1.2980,  0.4281, -1.2563, -1.8918, -0.7306],\n",
    "         [ 0.2820,  0.0810, -0.4398, -0.3574, -0.8132, -0.1044,  0.1898]],\n",
    "\n",
    "        [[-1.5765,  0.0495, -1.1407, -0.3947,  0.2474,  1.1779, -0.7999],\n",
    "         [ 1.5090,  0.3881, -0.0344,  0.6265,  0.0975,  0.3283, -0.2570],\n",
    "         [ 0.3752,  0.3056, -1.5868, -0.0883, -1.6226, -0.5940,  1.6271]],\n",
    "\n",
    "        [[ 1.1354, -0.3212,  0.4247,  0.0681,  0.9360, -0.3384,  1.4519],\n",
    "         [-0.1396, -0.8205, -0.1776,  0.4195,  0.2457, -0.9302,  1.4448],\n",
    "         [-0.7819, -0.9069, -1.4125, -0.3920, -1.0380,  2.0351,  0.6595]],\n",
    "\n",
    "        [[-1.2359,  0.3135, -0.1942,  0.2450, -1.0833, -0.1889,  0.2017],\n",
    "         [ 0.1240, -1.3459, -1.1797,  0.2006,  1.5969,  0.4993,  1.2332],\n",
    "         [ 0.1330,  0.2703,  0.5384,  2.3118, -0.3098, -0.0144,  0.7328]]])\n",
    ">>> c = torch.randn(2,7)\n",
    ">>> c\n",
    "tensor([[-0.1731,  1.9290,  0.1928,  0.6679,  0.7227, -1.8295,  0.8021],\n",
    "        [-1.0124,  1.5828,  1.6196, -0.1516,  0.9401,  0.2667, -1.2518]])\n",
    ">>> a_ = torch.einsum('ij,kjn, in -> ik', [a, b, c])\n",
    ">>> a_.size()\n",
    "torch.Size([2, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.  TreeQN\n",
    "einsum对于[TreeQN](https://openreview.net/pdf?id=H1dh6Ax0Z)中公式(6)的实现如下：\n",
    "给定$l$层的低维状态表达$z_l$和对于每一个action a对应的转移(激活)函数$W^a$, 使用residual connection残差链接来计算下一层$l+1$层的状态表达${z_{l+1}}^a$。\n",
    "$$\\mathbf{z}^a_{l+1} = \\mathbf{z}_l + \\tanh(\\mathbf{W}^a\\mathbf{z}_l)$$\n",
    "实践中，想要高效的在同一时间对于所有的转移函数(例如:所有的动作A)计算批大小为$B$中的$K$维状态表达$Z\\in\\mathbb R^{B\\,\\times\\,K}$，可以将这些转移函数放在张量$W\\in \\mathbb R^{A\\,\\times \\,K\\,\\times\\, K}$中并且计算高效的使用einsum来实现下一个状态表达。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def random_tensors(shape, num=1, requires_grad=False):\n",
    "  tensors = [torch.randn(shape, requires_grad=requires_grad) for i in range(0, num)]\n",
    "  return tensors[0] if num == 1 else tensors\n",
    "\n",
    "# Parameters\n",
    "# -- [num_actions x hidden_dimension], b is the parameter which need to be learned, so with gradient, W as the same\n",
    "b = random_tensors([5, 3], requires_grad=True)\n",
    "# -- [num_actions x hidden_dimension x hidden_dimension]\n",
    "W = random_tensors([5, 3, 3], requires_grad=True)\n",
    "\n",
    "def transition(zl):\n",
    "  # zl.unsqueeze(1) : convert zl to shape [2, 1, 3]\n",
    "  # einsum return a tensor of shape [2, 5, 3], b shape is [5, 3] , '+' operation  is broadcastable\n",
    "  # return shape [2, 5, 3] -- [batch_size x num_actions x hidden_dimension]\n",
    "  return zl.unsqueeze(1) + F.tanh(torch.einsum(\"bk,aki->bai\", [zl, W]) + b)\n",
    "\n",
    "# Sampled dummy inputs\n",
    "# -- [batch_size x hidden_dimension]\n",
    "zl = random_tensors([2, 3])\n",
    "\n",
    "zl_ = transition(zl).shape\n",
    ">>>torch.Size([2, 5, 3])\n",
    "tensor([[[-2.4080, -3.5068,  0.5025],\n",
    "         [-2.1035, -1.5754,  2.4678],\n",
    "         [-2.9639, -3.5726,  2.4945],\n",
    "         [-2.8011, -1.9178,  0.4953],\n",
    "         [-0.9763, -3.5736,  0.5021]],\n",
    "\n",
    "        [[ 2.0257, -2.7589,  0.1339],\n",
    "         [ 0.9365, -0.7853, -1.8133],\n",
    "         [ 0.6122, -2.7831,  0.1818],\n",
    "         [ 2.3801, -2.4952, -1.8072],\n",
    "         [ 1.7467, -2.7832,  0.1259]]], grad_fn=<AddBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
